{
    "summary": "The code sets up command line arguments for \"AppAgent,\" enables user actions selection or input gestures, validates inputs, performs corresponding actions with a controller object, logs data, handles errors, and displays recorded steps.",
    "details": [
        {
            "comment": "This code is setting up the command line arguments for an application called \"AppAgent\" which records human demonstrations of mobile app interactions. It checks if the app and demo names are provided, creates a directory to store the recorded data, and sets default values if any arguments are missing.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":0-36",
            "content": "import argparse\nimport datetime\nimport cv2\nimport os\nimport shutil\nimport sys\nimport time\nfrom and_controller import list_all_devices, AndroidController, traverse_tree\nfrom config import load_config\nfrom utils import print_with_color, draw_bbox_multi\narg_desc = \"AppAgent - Human Demonstration\"\nparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=arg_desc)\nparser.add_argument(\"--app\")\nparser.add_argument(\"--demo\")\nparser.add_argument(\"--root_dir\", default=\"./\")\nargs = vars(parser.parse_args())\napp = args[\"app\"]\ndemo_name = args[\"demo\"]\nroot_dir = args[\"root_dir\"]\nconfigs = load_config()\nif not app:\n    print_with_color(\"What is the name of the app you are going to demo?\", \"blue\")\n    app = input()\n    app = app.replace(\" \", \"\")\nif not demo_name:\n    demo_timestamp = int(time.time())\n    demo_name = datetime.datetime.fromtimestamp(demo_timestamp).strftime(f\"demo_{app}_%Y-%m-%d_%H-%M-%S\")\nwork_dir = os.path.join(root_dir, \"apps\")\nif not os.path.exists(work_dir):\n    os.mkdir(work_dir)"
        },
        {
            "comment": "Creating directories for storing demo and task files, checking if devices are attached.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":37-66",
            "content": "work_dir = os.path.join(work_dir, app)\nif not os.path.exists(work_dir):\n    os.mkdir(work_dir)\ndemo_dir = os.path.join(work_dir, \"demos\")\nif not os.path.exists(demo_dir):\n    os.mkdir(demo_dir)\ntask_dir = os.path.join(demo_dir, demo_name)\nif os.path.exists(task_dir):\n    shutil.rmtree(task_dir)\nos.mkdir(task_dir)\nraw_ss_dir = os.path.join(task_dir, \"raw_screenshots\")\nos.mkdir(raw_ss_dir)\nxml_dir = os.path.join(task_dir, \"xml\")\nos.mkdir(xml_dir)\nlabeled_ss_dir = os.path.join(task_dir, \"labeled_screenshots\")\nos.mkdir(labeled_ss_dir)\nrecord_path = os.path.join(task_dir, \"record.txt\")\nrecord_file = open(record_path, \"w\")\ntask_desc_path = os.path.join(task_dir, \"task_desc.txt\")\ndevice_list = list_all_devices()\nif not device_list:\n    print_with_color(\"ERROR: No device found!\", \"red\")\n    sys.exit()\nprint_with_color(\"List of devices attached:\\n\" + str(device_list), \"yellow\")\nif len(device_list) == 1:\n    device = device_list[0]\n    print_with_color(f\"Device selected: {device}\", \"yellow\")\nelse:\n    print_with_color(\"Please choose the Android device to start demo by entering its ID:\", \"blue\")"
        },
        {
            "comment": "Device input and check for valid device size.\nGet device resolution and print it, request user to specify demo goal.\nSave goal description in file.\nLabel interactive elements with red and blue numeric tags, clickable with red, scrollable with blue.\nLoop to capture screenshots and XML until errors occur.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":67-91",
            "content": "    device = input()\ncontroller = AndroidController(device)\nwidth, height = controller.get_device_size()\nif not width and not height:\n    print_with_color(\"ERROR: Invalid device size!\", \"red\")\n    sys.exit()\nprint_with_color(f\"Screen resolution of {device}: {width}x{height}\", \"yellow\")\nprint_with_color(\"Please state the goal of your following demo actions clearly, e.g. send a message to John\", \"blue\")\ntask_desc = input()\nwith open(task_desc_path, \"w\") as f:\n    f.write(task_desc)\nprint_with_color(\"All interactive elements on the screen are labeled with red and blue numeric tags. Elements \"\n                 \"labeled with red tags are clickable elements; elements labeled with blue tags are scrollable \"\n                 \"elements.\", \"blue\")\nstep = 0\nwhile True:\n    step += 1\n    screenshot_path = controller.get_screenshot(f\"{demo_name}_{step}\", raw_ss_dir)\n    xml_path = controller.get_xml(f\"{demo_name}_{step}\", xml_dir)\n    if screenshot_path == \"ERROR\" or xml_path == \"ERROR\":\n        break\n    clickable_list = []"
        },
        {
            "comment": "Code comments:\n\n1. Traverse the XML tree to find clickable and focusable elements (lines 92-104).\n2. Copy the clickable list to a new list called elem_list.\n3. Check if any focusable element is close to any clickable element based on distance threshold (configs[\"MIN_DIST\"]). If not, add it to elem_list (lines 107-114).\n4. Display the labeled image with bounding boxes for elements in elem_list using draw_bbox_multi function.\n5. Show the image and wait for user input (cv2 functions).\n6. Set user_input variable to \"xxx\" and print with colored output.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":92-115",
            "content": "    focusable_list = []\n    traverse_tree(xml_path, clickable_list, \"clickable\", True)\n    traverse_tree(xml_path, focusable_list, \"focusable\", True)\n    elem_list = clickable_list.copy()\n    for elem in focusable_list:\n        bbox = elem.bbox\n        center = (bbox[0][0] + bbox[1][0]) // 2, (bbox[0][1] + bbox[1][1]) // 2\n        close = False\n        for e in clickable_list:\n            bbox = e.bbox\n            center_ = (bbox[0][0] + bbox[1][0]) // 2, (bbox[0][1] + bbox[1][1]) // 2\n            dist = (abs(center[0] - center_[0]) ** 2 + abs(center[1] - center_[1]) ** 2) ** 0.5\n            if dist <= configs[\"MIN_DIST\"]:\n                close = True\n                break\n        if not close:\n            elem_list.append(elem)\n    labeled_img = draw_bbox_multi(screenshot_path, os.path.join(labeled_ss_dir, f\"{demo_name}_{step}.png\"), elem_list,\n                                  True)\n    cv2.imshow(\"image\", labeled_img)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n    user_input = \"xxx\"\n    print_with_col"
        },
        {
            "comment": "This code asks the user to choose an action from a list of options. It continues to ask for input until the correct option is chosen. If \"tap\" is chosen, it prompts for the element to tap by its numeric tag, then taps the element on screen and writes a record if successful.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":115-131",
            "content": "or(\"Choose one of the following actions you want to perform on the current screen:\\ntap, text, long \"\n                     \"press, swipe, stop\", \"blue\")\n    while user_input.lower() != \"tap\" and user_input.lower() != \"text\" and user_input.lower() != \"long press\" \\\n            and user_input.lower() != \"swipe\" and user_input.lower() != \"stop\":\n        user_input = input()\n    if user_input.lower() == \"tap\":\n        print_with_color(f\"Which element do you want to tap? Choose a numeric tag from 1 to {len(elem_list)}:\", \"blue\")\n        user_input = \"xxx\"\n        while not user_input.isnumeric() or int(user_input) > len(elem_list) or int(user_input) < 1:\n            user_input = input()\n        tl, br = elem_list[int(user_input) - 1].bbox\n        x, y = (tl[0] + br[0]) // 2, (tl[1] + br[1]) // 2\n        ret = controller.tap(x, y)\n        if ret == \"ERROR\":\n            print_with_color(\"ERROR: tap execution failed\", \"red\")\n            break\n        record_file.write(f\"tap({int(user_input)}):::{elem_list[int(user_input) - 1].uid}\\n\")"
        },
        {
            "comment": "This code segment allows the user to input text or simulate a long press on an element by choosing a numeric tag from 1 to the total number of elements in the list. It prompts for the input, validates the input, and performs the corresponding action using the controller object. The data is then recorded in a file with appropriate formatting.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":132-149",
            "content": "    elif user_input.lower() == \"text\":\n        print_with_color(f\"Which element do you want to input the text string? Choose a numeric tag from 1 to \"\n                         f\"{len(elem_list)}:\", \"blue\")\n        input_area = \"xxx\"\n        while not input_area.isnumeric() or int(input_area) > len(elem_list) or int(input_area) < 1:\n            input_area = input()\n        print_with_color(\"Enter your input text below:\", \"blue\")\n        user_input = \"\"\n        while not user_input:\n            user_input = input()\n        controller.text(user_input)\n        record_file.write(f\"text({input_area}:sep:\\\"{user_input}\\\"):::{elem_list[int(input_area) - 1].uid}\\n\")\n    elif user_input.lower() == \"long press\":\n        print_with_color(f\"Which element do you want to long press? Choose a numeric tag from 1 to {len(elem_list)}:\",\n                         \"blue\")\n        user_input = \"xxx\"\n        while not user_input.isnumeric() or int(user_input) > len(elem_list) or int(user_input) < 1:\n            user_input = input()"
        },
        {
            "comment": "The code is prompting the user for input to perform a long press or swipe action on an element from a list. It retrieves the bounding box coordinates, calculates the center point, performs the requested action, and logs the information if it was successful.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":150-166",
            "content": "        tl, br = elem_list[int(user_input) - 1].bbox\n        x, y = (tl[0] + br[0]) // 2, (tl[1] + br[1]) // 2\n        ret = controller.long_press(x, y)\n        if ret == \"ERROR\":\n            print_with_color(\"ERROR: long press execution failed\", \"red\")\n            break\n        record_file.write(f\"long_press({int(user_input)}):::{elem_list[int(user_input) - 1].uid}\\n\")\n    elif user_input.lower() == \"swipe\":\n        print_with_color(f\"What is the direction of your swipe? Choose one from the following options:\\nup, down, left,\"\n                         f\" right\", \"blue\")\n        user_input = \"\"\n        while user_input != \"up\" and user_input != \"down\" and user_input != \"left\" and user_input != \"right\":\n            user_input = input()\n        swipe_dir = user_input\n        print_with_color(f\"Which element do you want to swipe? Choose a numeric tag from 1 to {len(elem_list)}:\")\n        while not user_input.isnumeric() or int(user_input) > len(elem_list) or int(user_input) < 1:\n            user_input = input()"
        },
        {
            "comment": "This code takes user input to record a swipe action, writes it into a file along with the element's unique ID, and handles stopping the recording. If swipe execution fails, it prints an error message and breaks the loop. After completion, it displays the number of steps recorded in yellow color.",
            "location": "\"/media/root/Toshiba XG3/works/AppAgent/docs/src/scripts/step_recorder.py\":167-182",
            "content": "        tl, br = elem_list[int(user_input) - 1].bbox\n        x, y = (tl[0] + br[0]) // 2, (tl[1] + br[1]) // 2\n        ret = controller.swipe(x, y, swipe_dir)\n        if ret == \"ERROR\":\n            print_with_color(\"ERROR: swipe execution failed\", \"red\")\n            break\n        record_file.write(f\"swipe({int(user_input)}:sep:{swipe_dir}):::{elem_list[int(user_input) - 1].uid}\\n\")\n    elif user_input.lower() == \"stop\":\n        record_file.write(\"stop\\n\")\n        record_file.close()\n        break\n    else:\n        break\n    time.sleep(3)\nprint_with_color(f\"Demonstration phase completed. {step} steps were recorded.\", \"yellow\")"
        }
    ]
}